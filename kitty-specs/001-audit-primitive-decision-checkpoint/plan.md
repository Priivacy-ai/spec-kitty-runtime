# Implementation Plan: Mission Audit Primitive + Decision Checkpoint Plumbing

**Branch**: `001-audit-primitive-decision-checkpoint` | **Date**: 2026-02-26 | **Spec**: [spec.md](spec.md)
**Input**: `kitty-specs/001-audit-primitive-decision-checkpoint/spec.md`

## Summary

Add `AuditConfig` + `AuditStep` schema primitives to the mission YAML, extend the planner's DAG resolver to emit `decision_required` for blocking audit steps, wire the `provide_decision_answer` resume path for `audit:` prefixed decisions, and provide a `validate_mission_template_compatibility` diagnostics API with fixture YAML files for host-repo use.

The work is strictly additive within `schema.py`, `planner.py`, `engine.py`, and a new `diagnostics.py`; no 1.x compat surface is created.

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: pydantic>=2.0, pyyaml>=6.0, jsonschema>=4.0, spec-kitty-events==2.3.1
**Storage**: File-based run state (`.kittify/runtime/runs/<run_id>/state.json`)
**Testing**: pytest 8.x, deterministic fixtures (no mocks, no network)
**Target Platform**: Library (importable by host repos)
**Performance Goals**: All planner calls <5 ms (pure in-memory, offline)
**Constraints**: Offline-only, no network calls in any code path
**Scale/Scope**: Single library package; affects ~5 source files

## Project Structure

### Documentation (this feature)

```
kitty-specs/001-audit-primitive-decision-checkpoint/
├── spec.md          # Specification
├── plan.md          # This file
└── tasks.md         # Work packages (generated by /spec-kitty.tasks)
```

### Source Code Changes

```
src/spec_kitty_runtime/
├── schema.py         # + AuditConfig, AuditStep; extend MissionTemplate
├── planner.py        # + audit-aware DAG resolver + decision logic
├── engine.py         # + audit: prefix handling in provide_decision_answer
└── diagnostics.py    # NEW: validate_mission_template_compatibility

tests/
├── test_audit_schema.py        # AC-1, AC-2
├── test_audit_planner.py       # AC-3, AC-4, AC-7, AC-9
├── test_audit_engine.py        # AC-5, AC-6
├── test_compat_diagnostics.py  # AC-8
└── fixtures/
    ├── audit_valid_blocking.yaml      # blocking enforcement fixture
    ├── audit_valid_advisory.yaml      # advisory enforcement fixture
    ├── audit_mixed_steps.yaml         # steps + audit_steps combined
    ├── audit_only_steps.yaml          # audit_steps only (no regular steps)
    ├── audit_invalid_trigger.yaml     # bad trigger_mode → UNKNOWN_TRIGGER_MODE
    ├── audit_missing_config.yaml      # missing audit block → MISSING_AUDIT_CONFIG
    └── audit_bad_dependency.yaml      # broken depends_on → UNRESOLVED_DEPENDENCY
```

## Work Package Decomposition

### WP01 — AuditConfig + AuditStep schema

**Files**: `src/spec_kitty_runtime/schema.py`
**Tests**: `tests/test_audit_schema.py`

Add to `schema.py`:
- `AuditConfig(BaseModel)` with `trigger_mode: Literal["manual", "post_merge", "both"]` and `enforcement: Literal["advisory", "blocking"]`, plus optional `label: str | None` and `metadata: dict | None`. Both required fields — no defaults.
- `AuditStep(BaseModel)` with `id`, `title`, `description`, `audit: AuditConfig`, `depends_on: list[str]`. No `prompt`, `prompt_template`, or `requires_inputs`.
- Extend `MissionTemplate`: add `audit_steps: list[AuditStep] = Field(default_factory=list)`.
- Extend `load_mission_template_file`: parse `audit_steps` key; validate that at least one of `steps` or `audit_steps` is non-empty; raise `MissionRuntimeError` if neither.

**Acceptance**: AC-1, AC-2

---

### WP02 — Planner DAG extension for audit steps

**Files**: `src/spec_kitty_runtime/planner.py`
**Tests**: `tests/test_audit_planner.py`

Extend `plan_next` and `_resolve_next_step`:
- Build a **combined ordered sequence** of `PromptStep | AuditStep` objects:
  - Regular `steps` first (in template order)
  - `audit_steps` appended after, respecting `depends_on` for interleaving
  - DAG traversal uses the same dependency-check logic as existing steps
- When the resolved next step is an `AuditStep`:
  - If `enforcement == "advisory"`: emit `kind="step"` (agent executes informational)
  - If `enforcement == "blocking"`: emit `kind="decision_required"` with:
    - `decision_id = f"audit:{step.id}"`
    - `question = f"Audit checkpoint: {step.title}. Approve to continue?"`
    - `options = ["approve", "reject"]`
    - `input_key = None`

**Acceptance**: AC-3, AC-4, AC-7, AC-9

---

### WP03 — Engine provide_decision_answer audit: prefix

**Files**: `src/spec_kitty_runtime/engine.py`
**Tests**: `tests/test_audit_engine.py`

Extend `provide_decision_answer`:
- When `decision_id.startswith("audit:")`: validate answer is `"approve"` or `"reject"` (raise `MissionRuntimeError` otherwise)
- On `"approve"`: add audit step id to `completed_steps` (write to snapshot); delete from `pending_decisions`
- On `"reject"`: set `blocked_reason = f"Audit step '{step_id}' rejected by {actor.actor_id}"` in snapshot; delete from `pending_decisions`
- Emit `DECISION_INPUT_ANSWERED` event in both cases (existing event)

**Acceptance**: AC-5, AC-6

---

### WP04 — Compatibility diagnostics API + fixtures

**Files**: `src/spec_kitty_runtime/diagnostics.py` (new), `tests/test_compat_diagnostics.py`, `tests/fixtures/*.yaml`
**Tests**: `tests/test_compat_diagnostics.py`

New module `diagnostics.py`:
- `CompatibilityIssue(BaseModel)`: `code: str`, `field: str`, `message: str`, `severity: Literal["error", "warning"]`
- `CompatibilityReport(BaseModel)`: `path: str`, `is_compatible: bool`, `schema_valid: bool`, `audit_steps_valid: bool`, `issues: list[CompatibilityIssue]`, `warnings: list[str]`
- `validate_mission_template_compatibility(path: Path | str) -> CompatibilityReport`: runs all 8 validation checks from spec §3.5; returns report without raising exceptions

Fixture YAML files (7 total) as described in project structure above.

**Acceptance**: AC-8

---

## Complexity Tracking

No constitution violations. This is a purely additive, offline, single-library change with no external dependencies added.

## Dependency Graph

```
WP01 (schema)
  └─> WP02 (planner)
        └─> WP03 (engine)
WP01 ──> WP04 (diagnostics, independent of WP02/WP03)
```

WP04 can be developed in parallel with WP02 and WP03 after WP01 is done.
